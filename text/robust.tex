
To show the gaussian variation in the data we collect, figure \refFigure{fig:gauss} depicts a scatter plot of $1000$ sequential runs of the program \bzip, after being compiled using the \funcname{Static} inliner (\llvm) whose input was {\tt ebooks}. The figure shows a gaussian noise around the median plus a few outliers, generated by the operating system regular use. These outliers are filtered off from the data we are using. They are easily discarded because they have much more variance (more than one deviation from the median).

\begin{figure}
  \centering
  \includegraphics[width=1.00\linewidth]{Figures/nt1000}
  \caption{Running $1000$ times the same program with the same input data}
  \label{fig:gauss}
\end{figure}

We ran three independent experiments, the first is the $10$-times experiment, then $100$-times and, after that, $1000$-times, because we needed to confirm that there is no difference on their means, and also that we can discard the outliers. To make sure that we have robust measures, we ran some simple statistics to know the mean, the median, the standard-deviation from the mean (std-mean), and the standard-deviation from the median (std-median), shown in \refTable{tab:robustTest}. We also ran t-tests on each sample pairs to verify if their means were the same, the results for the t-tests are shown in \refTable{tab:ttest} below.

\begin{table}
  \centering
  \begin{tiny}
  \input{Tables/robustTest}
  \end{tiny}
  \caption{Simple statistics on the experiment}
  \label{tab:robustTest}
\end{table}

The t-tests in \refTable{tab:ttest} show that the null hypothesis cannot be discarded, as the value $0$ in each line of the \emph{t-test} column confirms. The \emph{p-values} illustrate the confidence in the hypothesis, in this case, that the means are different.

\begin{table}
  \centering
  \begin{tiny}
  \input{Tables/tTest}
  \end{tiny}
  \caption{t-tests applied pairwise to the $10$, $100$, and $1000$ runs}
  \label{tab:ttest}
\end{table}

%\rr{Probably running 3-fold will not be enough, if the running time for a data set is much higher than the running time of this exercise, we have to expect much more noise in the data, but it will be embedded in the data, therefore impossible to extract directly. The only countermeasure is to increase the number of times we run each program from 3 to 5, maybe this could be enough.}

Our experiments have also shown that the variance when running the same data just three times in a row is not quite different from the one running $100$ times. When we consider each `input-run' a $3$-fold run -- which means we ran $300$ times the same experiment --, and we consider a `full-run' as running a $3$-fold run to each input. We ran $100$ `full-runs' in this experiment, and we put some extra noise at its end.

What this means is that even though the effect of the noise can mask the correct values, we can treat them in order to assure robustness. This is the way we employed to empirically verify the soundness of the \CP\ methodology. As figure \refFigure{fig:CProbust} below shows, the deviation from the mean is not large, but here is a subtle knob increasing the running time of the all programs in the experiment by its end. It was caused by the execution of another system at the same time competing for the same resources. In (\refFigure{CP:ebooks}) we show in the $y$-axis the running time for each program at each 3-fold run, which are shown on the $x$-axis. It can be also visualised in the histogram (\refFigure{CP:hist}). These figures show the $3$-fold run for the input data {\tt ebooks}, where in the $x$-axis we show the running time for the program and on the $y$-axis we show the number of runs at each bin.

\begin{figure}
  \centering
  \begin{minipage}[t]{\linewidth}
    \subfigure[$100$-time runs of the $3$-fold execution of input {\tt ebooks} for program \bzip] {
      \begin{minipage}[b]{0.75\textwidth}
        \centering
        \includegraphics[height=12em]{Figures/ebooks300}
      \end{minipage}
      \label{CP:ebooks}
    }
    \vspace{1em}
    \hrule
    \vspace{1em}
    \subfigure[Histogram for the {\tt auriel} input] {
      \begin{minipage}[b]{0.75\textwidth}
        \centering
        \includegraphics[height=12em]{Figures/ebooks}
      \end{minipage}
      \label{CP:hist}
    }
  \end{minipage}
  \caption{$100$-times running $3$-fold experiment}
  \label{fig:CProbust}
\end{figure}

The figures \refFigure{fig:CProbust} and \refFigure{fig:gauss} show that collecting data from single execution can show erroneous results, even using machines with no other running program, we still have some noise due to operating system activities, interruptions, etc. And also that a simple inclusion of a simple job during the running cycle can perturb the execution time, as can be observed by the knob in figure \refFigure{fig:CProbust}.

The robustness is achieved when we can statiscally assure that the variance on the data is not large.
The data used in the experiment are shown in \refTable{tab:simStats}, and the deviations from the mean (and median) to each $3$-fold run are summarized as the average, minimum, and maximum values, all found on the $300$-times experiment.

\begin{table}
  \centering
  \begin{tiny}
  \input{Tables/simStats}
  \end{tiny}
  \caption{Deviation from the mean and from the median in the experiment}
  \label{tab:simStats}
\end{table}

We also ran the t-tests to show that the means are statistically representing the same distribution. This is summarized in \refTable{tab:statTest} below. We can see very little outliers, except for knob region, because the runtime was being raised during certain amount of time forcing a gradient increasing the time values, and after it what happened was the other way around, decreasing the time values. Both tables \refTable{tab:simStats} and \refTable{tab:statTest} are shown for the runs.

\begin{table}
  \centering
  \begin{tiny}
  \input{Tables/statTest}
  \end{tiny}
  \caption{Test on the means}
  \label{tab:statTest}
\end{table}

This experiment brought us confidence in the machine learning method we devised to tune-in the compiler parameters. We considered the possibility of increasing the number of times each individual run need to be performed, in order to achieve low variance in the data; hence we could trust the results. As this experiment has shown, the $3$-fold run is a good choice, because it does not penalize much the total running time. Also, was shown that single-run testbeds are error-prone because they doesn't take the variance into account. %We show these data in \refTable{tab:runTime}.\rr{this table is still dummy}

%\begin{table}
%  \centering
%  \begin{tiny}
%  \input{Tables/runTime}
%  \end{tiny}
%  \caption{Running time of experiments, considering $3$-times run}
%  \label{tab:runTime}
%\end{table}

%If we had to run more than three times, say $n$ times, the total running time would be increased for a factor of $k$, where $k = \frac{n}{3}$, which, in case $n = 9$, would result in three times the total running time we had in this experiment.
